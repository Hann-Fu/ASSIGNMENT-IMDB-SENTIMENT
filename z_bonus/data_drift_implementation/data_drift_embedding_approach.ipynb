{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Abstraction**\n",
    "\n",
    "This file show the feasibility and implementation of detecting data drift for text, using embedding models to vectorize the text then calculate the cosine similarity by a single dot product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"protostarss/distilbert_imdb_full\"\n",
    "# Download the model\n",
    "model_path = snapshot_download(repo_id=repo_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "\n",
    "def get_bert_embedding(text: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate embeddings for the input text using DistilBERT base model without classification head.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to generate embeddings for.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    # Tokenize and prepare input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate embeddings using base model without classification head\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Use the [CLS] token embedding (first token) from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the IMDB dataset from Kaggle\n",
    "df_imbd = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df_tmbd = pd.read_csv(\"TMDB Dataset.csv\")\n",
    "\n",
    "df_imbd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB sample 1 shape: (50, 2)\n",
      "IMDB sample 2 shape: (50, 2)\n",
      "TMDB sample shape: (99, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 100 rows from df_imbd and df_tmbd, df_imbd do 3 times\n",
    "# Sample 50 rows from each dataset\n",
    "imbd_sample_1 = df_imbd.sample(n=50, random_state=66)\n",
    "imbd_sample_2 = df_imbd.sample(n=50, random_state=99)\n",
    "\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"IMDB sample 1 shape: {imbd_sample_1.shape}\")\n",
    "print(f\"IMDB sample 2 shape: {imbd_sample_2.shape}\")\n",
    "print(f\"TMDB sample shape: {df_tmbd.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarities:\n",
      "IMDB Sample 1 vs IMDB Sample 2: 0.9630\n",
      "IMDB Sample 1 vs TMDB: 0.8859\n",
      "IMDB Sample 2 vs TMDB: 0.7540\n"
     ]
    }
   ],
   "source": [
    "# Calculate the embedding of these three datasets, and then calculate the cosine similarity between the embeddings of the three datasets\n",
    "def get_bert_embedding(text: str, model, tokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate BERT embeddings for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to generate embeddings for.\n",
    "        model: The BERT model to use for generating embeddings.\n",
    "        tokenizer: The tokenizer to use for preprocessing the text.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    # Tokenize and prepare input\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate embeddings using base model without classification head\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # Use the [CLS] token embedding (first token) from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def calculate_dataset_embeddings(df: pd.DataFrame, text_column: str, model, tokenizer) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate mean embeddings for a dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing the text data.\n",
    "        text_column (str): Name of the column containing text to embed.\n",
    "        model: The BERT model to use for generating embeddings.\n",
    "        tokenizer: The tokenizer to use for preprocessing the text.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mean embedding vector for the dataset.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process each text in the dataset\n",
    "    for text in df[text_column]:\n",
    "        embedding = get_bert_embedding(text, model, tokenizer)\n",
    "        all_embeddings.append(embedding)\n",
    "    \n",
    "    # Stack all embeddings and calculate mean\n",
    "    stacked_embeddings = torch.stack(all_embeddings)\n",
    "    mean_embedding = torch.mean(stacked_embeddings, dim=0)\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "# Calculate embeddings for each dataset\n",
    "imbd_embeddings_1 = calculate_dataset_embeddings(imbd_sample_1, 'review', model, tokenizer)\n",
    "imbd_embeddings_2 = calculate_dataset_embeddings(imbd_sample_2, 'review', model, tokenizer)\n",
    "tmbd_embeddings = calculate_dataset_embeddings(df_tmbd, 'reviews', model, tokenizer)\n",
    "\n",
    "# Calculate cosine similarity between embeddings\n",
    "def cosine_similarity(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two tensors.\n",
    "\n",
    "    Args:\n",
    "        a (torch.Tensor): First tensor.\n",
    "        b (torch.Tensor): Second tensor.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Ensure tensors are 2D with shape [1, dim]\n",
    "    a = a.reshape(1, -1)\n",
    "    b = b.reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = torch.nn.functional.cosine_similarity(a, b, dim=1)\n",
    "    return similarity.item()\n",
    "\n",
    "# Calculate and print similarities\n",
    "print(\"\\nCosine Similarities:\")\n",
    "print(f\"IMDB Sample 1 vs IMDB Sample 2: {cosine_similarity(imbd_embeddings_1, imbd_embeddings_2):.4f}\")\n",
    "print(f\"IMDB Sample 1 vs TMDB: {cosine_similarity(imbd_embeddings_1, tmbd_embeddings):.4f}\")\n",
    "print(f\"IMDB Sample 2 vs TMDB: {cosine_similarity(imbd_embeddings_2, tmbd_embeddings):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:  \n",
    "We can see that samples from IMDB are quite similar to each other. However, the similarity score between TMDB and IMDB is low. This means the meaning and style of TMDB reviews are different from IMDB ones — this is called data drift. So, using **text embeddings** to detect **data drift** is a good and reasonable method in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coding-test-9uclmUVG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
